# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LAt3nyresGMSOfIZu8GyTKXvt1habAW0
"""

# app.py

import streamlit as st
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os

# ===========================
# CONFIG
# ===========================
DB_DIR = "chroma_db"
DEFAULT_DOCS = "hr_pdfs"
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
LLM_MODEL_ID = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# ===========================
# LOAD LLM
# ===========================
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID, use_auth_token=True)
    tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(
        LLM_MODEL_ID,
        torch_dtype=torch.float32,
        device_map="auto",
        use_auth_token=True
    )
    return model, tokenizer

# ===========================
# RAG: Setup VectorStore
# ===========================
@st.cache_resource
def setup_vectorstore():
    if not os.path.exists(DB_DIR):
        os.makedirs(DB_DIR)
        loaders = []
        for file in os.listdir(DEFAULT_DOCS):
            if file.endswith(".pdf"):
                path = os.path.join(DEFAULT_DOCS, file)
                loaders.append(PyPDFLoader(path))

        docs = []
        for loader in loaders:
            docs.extend(loader.load())

        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)
        chunks = splitter.split_documents(docs)

        embed = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
        vectordb = Chroma.from_documents(chunks, embed, persist_directory=DB_DIR)
        vectordb.persist()
    else:
        vectordb = Chroma(persist_directory=DB_DIR, embedding_function=HuggingFaceEmbeddings(model_name=EMBED_MODEL))

    return vectordb.as_retriever(search_kwargs={"k": 2})

# ===========================
# Generate Answer
# ===========================
def generate_answer(prompt, model, tokenizer):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=200,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def build_prompt(context, question):
    return f"""You are an HR assistant. Only use the context to answer. Be concise.

Context:
{context}

Question: {question}

Answer:"""

# ===========================
# UI Starts Here
# ===========================
st.set_page_config("HR Assistant", layout="wide")
st.title("ðŸ§  AI-Powered HR Assistant")

page = st.sidebar.selectbox("Navigation", ["ðŸ“˜ About", "ðŸ¤– Ask HR", "ðŸ“¤ Upload Your Own PDF"])

# Load model/vector
model, tokenizer = load_model()
retriever = setup_vectorstore()

if page == "ðŸ“˜ About":
    st.subheader("About this AI")
    st.markdown("""
    This is an AI-powered HR Assistant built with:
    - ðŸ§  Retrieval-Augmented Generation (RAG)
    - ðŸ¤– TinyLlama 1.1B Model
    - ðŸ“„ ChromaDB Vector Store
    - ðŸ§© LangChain
    - ðŸ§° Streamlit for UI

    You can ask any HR-related questions, or upload your own company documents and query them.
    """)

elif page == "ðŸ¤– Ask HR":
    st.subheader("Ask Your HR Questions")
    query = st.text_input("Enter your question:")

    if query:
        with st.spinner("Fetching answer..."):
            docs = retriever.get_relevant_documents(query)
            context = "\n".join([doc.page_content for doc in docs])
            prompt = build_prompt(context, query)
            answer = generate_answer(prompt, model, tokenizer)
            st.markdown(f"**Answer:** {answer}")

elif page == "ðŸ“¤ Upload Your Own PDF":
    st.subheader("Upload a PDF")
    uploaded_file = st.file_uploader("Upload a PDF file to include in the knowledge base", type=["pdf"])

    if uploaded_file is not None:
        # Save uploaded file to the designated directory
        file_path = os.path.join(DEFAULT_DOCS, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.read())
        st.success(f"Uploaded! Please **reload** the app to embed the new document into the knowledge base.")